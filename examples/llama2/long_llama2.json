{
    "name": "long_llama2",
    "num_layers": 80,
    "n_head": 64,
    "hidden_dim": 8192,
    "vocab_size": 32000,
    "max_seq_len": 409600,
    "num_key_value_heads": 8,
    "ffn_embed_dim": 28672,
    "model_type": "llama",
    "mlp_gated_linear_units": true
}
